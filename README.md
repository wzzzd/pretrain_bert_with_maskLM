# Pretrain_Bert_with_MaskLM

## Info
ä½¿ç”¨Mask LMé¢„è®­ç»ƒä»»åŠ¡æ¥é¢„è®­ç»ƒBertæ¨¡å‹ã€‚

åŸºäºpytorchæ¡†æ¶ï¼Œè®­ç»ƒå…³äºå‚ç›´é¢†åŸŸè¯­æ–™çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç›®çš„æ˜¯æå‡ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚




## Pretraining Task
Mask Language Modelï¼Œç®€ç§°Mask LMï¼Œå³åŸºäºMaskæœºåˆ¶çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚

åŒæ—¶æ”¯æŒ åŸç”Ÿçš„`MaskLM`ä»»åŠ¡å’Œ`Whole Words Masking`ä»»åŠ¡ã€‚é»˜è®¤ä½¿ç”¨`Whole Words Masking`ã€‚

#### MaskLM
ä½¿ç”¨æ¥è‡ªäºBertçš„maskæœºåˆ¶ï¼Œå³å¯¹äºæ¯ä¸€ä¸ªå¥å­ä¸­çš„è¯ï¼ˆtokenï¼‰ï¼š
* 85%çš„æ¦‚ç‡ï¼Œä¿ç•™åŸè¯ä¸å˜
* 15%çš„æ¦‚ç‡ï¼Œä½¿ç”¨ä»¥ä¸‹æ–¹å¼æ›¿æ¢
    * 80%çš„æ¦‚ç‡ï¼Œä½¿ç”¨å­—ç¬¦`[MASK]`ï¼Œæ›¿æ¢å½“å‰tokenã€‚
    * 10%çš„æ¦‚ç‡ï¼Œä½¿ç”¨è¯è¡¨éšæœºæŠ½å–çš„tokenï¼Œæ›¿æ¢å½“å‰tokenã€‚
    * 10%çš„æ¦‚ç‡ï¼Œä¿ç•™åŸè¯ä¸å˜ã€‚
    <!-- * ![](./picture/mask_method.png) -->
    * <img src=./picture/mask_method.png width=50% />

#### Whole Words Masking
ä¸MaskLMç±»ä¼¼ï¼Œä½†æ˜¯åœ¨maskçš„æ­¥éª¤æœ‰äº›å°‘ä¸åŒã€‚

åœ¨Bertç±»æ¨¡å‹ä¸­ï¼Œè€ƒè™‘åˆ°å¦‚æœå•ç‹¬ä½¿ç”¨æ•´ä¸ªè¯ä½œä¸ºè¯è¡¨çš„è¯ï¼Œé‚£è¯è¡¨å°±å¤ªå¤§äº†ã€‚ä¸åˆ©äºæ¨¡å‹å¯¹åŒç±»è¯çš„ä¸åŒå˜ç§çš„ç‰¹å¾å­¦ä¹ ï¼Œæ•…é‡‡ç”¨äº†WordPieceçš„æ–¹å¼è¿›è¡Œåˆ†è¯ã€‚

`Whole Words Masking`çš„æ–¹æ³•åœ¨äºï¼Œåœ¨è¿›è¡Œmaskæ“ä½œæ—¶ï¼Œå¯¹è±¡å˜ä¸ºåˆ†è¯å‰çš„æ•´ä¸ªè¯ï¼Œè€Œéå­è¯ã€‚


## Model
ä½¿ç”¨åŸç”Ÿçš„Bertæ¨¡å‹ä½œä¸ºåŸºå‡†æ¨¡å‹ã€‚
* ![](./picture/bert_architecture.png)



## Datasets
é¡¹ç›®é‡Œçš„æ•°æ®é›†æ¥è‡ª`wikitext`ï¼Œåˆ†æˆä¸¤ä¸ªæ–‡ä»¶è®­ç»ƒé›†ï¼ˆtrain.txtï¼‰å’Œæµ‹è¯•é›†ï¼ˆtest.txtï¼‰ã€‚

æ•°æ®ä»¥è¡Œä¸ºå•ä½å­˜å‚¨ã€‚

è‹¥æƒ³è¦æ›¿æ¢æˆè‡ªå·±çš„æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œæ›¿æ¢ã€‚ï¼ˆæ³¨æ„ï¼šå¦‚æœæ˜¯é¢„è®­ç»ƒä¸­æ–‡æ¨¡å‹ï¼Œéœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶`Config.py`ä¸­çš„`self.initial_pretrain_model`å’Œ`self.initial_pretrain_tokenizer`ï¼Œå°†å€¼ä¿®æ”¹æˆ `bert-base-chinese`ï¼‰

è‡ªå·±çš„æ•°æ®é›†ä¸éœ€è¦åšmaskæœºåˆ¶å¤„ç†ï¼Œä»£ç ä¼šå¤„ç†ã€‚


## Training Target
æœ¬é¡¹ç›®ç›®çš„åœ¨äºåŸºäºç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œå¦‚googleå¼€æºçš„`bert-base-uncased`ã€`bert-base-chinese`ç­‰ï¼Œåœ¨å‚ç›´é¢†åŸŸçš„æ•°æ®è¯­æ–™ä¸Šï¼Œå†æ¬¡è¿›è¡Œé¢„è®­ç»ƒä»»åŠ¡ï¼Œç”±æ­¤æå‡bertçš„æ¨¡å‹è¡¨å¾èƒ½åŠ›ï¼Œæ¢å¥è¯è¯´ï¼Œä¹Ÿå°±æ˜¯æå‡ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚


## Environment

é¡¹ç›®ä¸»è¦ä½¿ç”¨äº†Huggingfaceçš„`datasets`ã€`transformers`æ¨¡å—ï¼Œæ”¯æŒCPUã€å•å¡å•æœºã€å•æœºå¤šå¡ä¸‰ç§æ¨¡å¼ã€‚

pythonçš„ç‰ˆæœ¬ä¸º: 3.8

å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–åŒ…
```
    pip install -r requirement.txt
```
ä¸»è¦åŒ…å«çš„æ¨¡å—å¦‚ä¸‹ï¼š
```
    numpy==1.24.1
    pandas==1.5.2
    scikit_learn==1.2.1
    torch==1.8.0
    tqdm==4.64.1
    transformers==4.26.1
```



## Get Start

### å•å¡æ¨¡å¼
(1) è®­ç»ƒ

ç›´æ¥è¿è¡Œ
```
    python main.py
```

(2) æµ‹è¯•

ä¿®æ”¹`Config.py`æ–‡ä»¶ä¸­çš„`self.mode='test'`ï¼Œå†è¿è¡Œ
```
    python main.py
```

### å¤šå¡æ¨¡å¼ï¼ˆè®­ç»ƒï¼‰
å¦‚æœä½ è¶³å¤Ÿå¹¸è¿ï¼Œæ‹¥æœ‰äº†å¤šå¼ GPUå¡ï¼Œé‚£ä¹ˆæ­å–œä½ ï¼Œä½ å¯ä»¥è¿›å…¥èµ·é£æ¨¡å¼ã€‚ğŸš€ğŸš€

ï¼ˆ1ï¼‰ä½¿ç”¨torchçš„`nn.parallel.DistributedDataParallel`æ¨¡å—è¿›è¡Œå¤šå¡è®­ç»ƒã€‚å…¶ä¸­`config.py`æ–‡ä»¶ä¸­å‚æ•°å¦‚ä¸‹ï¼Œé»˜è®¤å¯ä»¥ä¸ç”¨ä¿®æ”¹ã€‚

* <font color=#009393>`self.cuda_visible_devices`è¡¨ç¤ºç¨‹åºå¯è§çš„GPUå¡å·ï¼Œç¤ºä¾‹ï¼š`1,2`â†’å¯åœ¨GPUå¡å·ä¸º1å’Œ2ä¸Šè·‘ï¼Œäº¦å¯ä»¥æ”¹å¤šå¼ ï¼Œå¦‚`0,1,2,3`ã€‚</font>
* <font color=#009393>`self.device`åœ¨å•å¡æ¨¡å¼ï¼Œè¡¨ç¤ºç¨‹åºè¿è¡Œçš„å¡å·ï¼›åœ¨å¤šå¡æ¨¡å¼ä¸‹ï¼Œè¡¨ç¤ºmasterçš„ä¸»å¡ï¼Œé»˜è®¤ä¼šå˜æˆä½ æŒ‡å®šå¡å·çš„ç¬¬ä¸€å¼ å¡ã€‚è‹¥åªæœ‰cpuï¼Œé‚£ä¹ˆå¯ä¿®æ”¹ä¸º`cpu`ã€‚</font>
* <font color=#009393>`self.port`è¡¨ç¤ºå¤šå¡æ¨¡å¼ä¸‹ï¼Œè¿›ç¨‹é€šä¿¡å ç”¨çš„ç«¯å£å·ã€‚ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰</font>
* <font color=#009393>`self.init_method`è¡¨ç¤ºå¤šå¡æ¨¡å¼ä¸‹è¿›ç¨‹çš„é€šè®¯åœ°å€ã€‚ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰</font>
* <font color=#009393>`self.world_size`è¡¨ç¤ºå¯åŠ¨çš„è¿›ç¨‹æ•°é‡ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰ã€‚åœ¨torch==1.3.0ç‰ˆæœ¬ä¸‹ï¼Œåªéœ€æŒ‡å®šä¸€ä¸ªè¿›ç¨‹ã€‚åœ¨1.9.0ä»¥ä¸Šï¼Œéœ€è¦ä¸GPUæ•°é‡ç›¸åŒã€‚</font>


ï¼ˆ2ï¼‰è¿è¡Œç¨‹åºå¯åŠ¨å‘½ä»¤
```
    chmod 755 run.sh
    ./run.sh
```

# Experiment

## training
ä½¿ç”¨äº¤å‰ç†µï¼ˆcross-entropyï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå›°æƒ‘åº¦ï¼ˆperplexityï¼‰å’ŒLossä½œä¸ºè¯„ä»·æŒ‡æ ‡æ¥è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š
<!-- ![](./picture/experiment.png) -->
<img src=./picture/experiment.png width=70% />

## test
ç»“æœä¿å­˜åœ¨`dataset/output/pred_data.csv`ï¼Œåˆ†åˆ«åŒ…å«ä¸‰åˆ—ï¼š
- `src`è¡¨ç¤ºåŸå§‹è¾“å…¥
- `pred`è¡¨ç¤ºæ¨¡å‹é¢„æµ‹
- `mask`è¡¨ç¤ºæ¨¡å‹è¾“å…¥ï¼ˆå¸¦æœ‰maskå’Œpadç­‰tokenï¼‰

ç¤ºä¾‹

```
src:  [CLS] art education and first professional work [SEP]
pred: [CLS] art education and first class work [SEP]
mask: [CLS] art education and first [MASK] work [SEP] [PAD] [PAD] [PAD] ...
```


# Reference

ã€Bertã€‘[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)

ã€transformersã€‘[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

ã€datasetsã€‘[https://huggingface.co/docs/datasets/quicktour.html](https://huggingface.co/docs/datasets/quicktour.html)




